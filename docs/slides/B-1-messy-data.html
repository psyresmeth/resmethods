<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Getting back into R - and how to handle messy data</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-webcam/webcam.js"></script>
    <script id="xaringanExtra-webcam-options" type="application/json">{"width":"200","height":"200","margin":"1em"}</script>
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <link href="libs/xaringanExtra-extra-styles/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="css/my-theme.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Getting back into R - and how to handle messy data
]
.subtitle[
## Advanced Research Methods and Skills
]
.date[
### 2023/02/28
]

---




# Attendance Code

Please use this code: **160057** to register your attendance. 

---
# Introductions

.pull-left[
| Dr. Sarah Sauve |
|------------------------------|
| ![A photograph of Dr. Sarah Sauve](images/01/Sarah.jpg) |
|Sarah Swift Building Room 4227|
|ssauve@lincoln.ac.uk       |
|Office hours: Tues 12-2     |
]
.pull-right[
| Dr. Peter Clark |
|------------------------------|
| ![A photograph of Dr Peter Clark](images/01/Peter.jpg) |
|Sarah Swift Building Room 3211|
|pclark@lincoln.ac.uk       |
|Office hours: Thurs 10-11   |
]
---
# Topics previously covered

- Data visualization
    - Using **ggplot2**
    
- Data manipulation
    - Using **dplyr**

- Basic statistics
    - t-tests, correlations
    - regression
    - ANOVA

---
# Themes for today

.pull-left[
- Refamiliarizing yourself with R

- Basic data transformations

- Missing data
]
.pull-right[
![:scale 70%](images/RStudioCloud_proj_circ.png)
![:scale 70%](images/tidy-1.png)
]

???

Tidy data figure from https://r4ds.had.co.nz/tidy-data.html, distributed under a CC BY-NC-ND 3.0 US license

---
# Tabular data

.pull-left[

```
## # A tibble: 16 Ã— 4
##    Participant Viewpoint Block     RT
##          &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt;
##  1           1 Different First   571.
##  2           1 Different Second  443.
##  3           1 Same      First   559.
##  4           1 Same      Second  411.
##  5           2 Different First   482.
##  6           2 Different Second  356.
##  7           2 Same      First   530.
##  8           2 Same      Second  352.
##  9           3 Different First   416.
## 10           3 Different Second  424.
## 11           3 Same      First   486.
## 12           3 Same      Second  383.
## 13           4 Different First   538.
## 14           4 Different Second  383.
## 15           4 Same      First   533.
## 16           4 Same      Second  414.
```
]
.pull-right[
Tables of data are what you're most commonly dealing with in R.

This one conforms to the **tidy data** principles:

One row per observation, one column per variable

![:scale 80%](images/tidy-1.png)
]

???

Tidy data figure from https://r4ds.had.co.nz/tidy-data.html, distributed under a CC BY-NC-ND 3.0 US license

---
# Different types of file

The most common file formats you'll deal with are either Excel files or text files, but you may also find dealing with SPSS files useful.

Fortunately, R has several functions and packages for importing data!

|File formats| File extension| Functions| Package|
|-|-|-|-|
|SPSS  | .sav| `read_sav()`| library(haven)|
|Excel | .xls, .xlsx|`read_excel()`|library(readxl)|
|Text  | .csv, .txt, .* |`read_csv()`, `read_delim()`|library(readr)|


---
background-image: url(../images/dplyr-logo.png)
background-size: 6%
background-position: 90% 5%
# Data wrangling

`dplyr` is a really useful package for manipulation of data tables.

|Function |Effect|
|------------|----|
| select()   |Include or exclude variables (columns)|
| arrange()  |Change the order of observations (rows)|
| filter()   |Include or exclude observations (rows)|
| mutate()   |Create new variables (columns)|
| group_by() |Create groups of observations|
| summarise()|Aggregate or summarise groups of observations (rows)|

---
background-image: url(../images/ggplot2-logo.png)
background-size: 8%
background-position: 85% 5%
# Quickly plotting your data

.pull-left[
Plottting data is really important for all aspects of data analysis.

The `ggplot2` package provides a framework for doing this:

```r
ggplot(example_rt_df,
       aes(x = RT, fill = Block)) +
  geom_density(alpha = 0.5) +
  theme_bw()
```
]
.pull-right[
![](B-1-messy-data_files/figure-html/dens-plot-1.png)
]

---
# Running statistics

The humble t-test...


```r
t.test(RT~Block, data = example_rt_df)
```

```
## 
## 	Welch Two Sample t-test
## 
## data:  RT by Block
## t = 16.167, df = 197.71, p-value &lt; 2.2e-16
## alternative hypothesis: true difference in means between group First and group Second is not equal to 0
## 95 percent confidence interval:
##   90.58705 115.75737
## sample estimates:
##  mean in group First mean in group Second 
##             501.1781             398.0059
```

---
# Running statistics

The factorial ANOVA... (the `aov_ez()` function from the `afex` package)


```r
aov_ez(dv = "RT",
       within = c("Block", "Viewpoint"),
       id = "Participant",
       data = example_rt_df)
```

```
## Anova Table (Type 3 tests)
## 
## Response: RT
##            Effect    df     MSE          F  ges p.value
## 1           Block 1, 49 2035.31 261.50 *** .570   &lt;.001
## 2       Viewpoint 1, 49 2094.40       0.62 .003    .435
## 3 Block:Viewpoint 1, 49 1795.34       0.49 .002    .486
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1
```

---
class: center, inverse, middle
# How to handle "messy" or otherwise awkward data

---
# The ideal data

In an ideal world all our data would be beautifully normal:


```
## `geom_smooth()` using formula = 'y ~ x'
```

![](B-1-messy-data_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;


---
# The real data

But reality is rarely so kind. Data can be all kinds of messy. It can have *outliers*...


```
## `geom_smooth()` using formula = 'y ~ x'
```

![](B-1-messy-data_files/figure-html/outlier-plot-1.png)&lt;!-- --&gt;

---
# The real data

Data can be *missing*...
.pull-left[

```
## `geom_smooth()` using formula = 'y ~ x'
```

![](B-1-messy-data_files/figure-html/missing-plot-1.png)&lt;!-- --&gt;
]
.pull-right[

```
##          X1        X2
## 1        NA  8.821568
## 2        NA  9.653133
## 3        NA  9.629883
## 4        NA 10.550607
## 5        NA 11.181548
## 6  10.07902        NA
## 7  10.86060        NA
## 8  10.03637  8.852198
## 9  10.44595 10.472557
## 10 10.57747 10.614811
```
Complete cases = 193
]

---
# The real data

Data can be *skewed*...

![](B-1-messy-data_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;

---
# The real data

.pull-left[
![](B-1-messy-data_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;
]
.pull-right[
There can be any combination of these things...

All of these pose problems for estimating the properties of our data, the relationships between variables, and the phenomena we are investigating.
]

---
class: center, inverse, middle
# Handling outliers

---
# What is an outlier?

Two out of the 200 pairs of x-y values were replaced. 

The resulting coefficient (approx *r* = .49) is *way-off* the true coefficient for these data. 

.pull-left[

```
## `geom_smooth()` using formula = 'y ~ x'
```

![](B-1-messy-data_files/figure-html/unnamed-chunk-7-1.png)&lt;!-- --&gt;
]
.pull-right[

```
## `geom_smooth()` using formula = 'y ~ x'
```

![](B-1-messy-data_files/figure-html/unnamed-chunk-8-1.png)&lt;!-- --&gt;
]

---
# What is an outlier?

.pull-left[
The problem gets even worse with smaller sample sizes.

Here there are 50 datapoints with two outliers, rather than 200.

The correlation coefficient becomes even *more* biased than it was previously.
]

.pull-right[

```
## `geom_smooth()` using formula = 'y ~ x'
```

![](B-1-messy-data_files/figure-html/unnamed-chunk-9-1.png)&lt;!-- --&gt;
]

---
# What should we do with outliers?

Three common approaches:

1. **Remove them**
    
    - If you're sure these reflect an error, not genuine data, then removal is a possibility. 

--
2. **Transformation**
    
    - *rescaling* or *transforming* your data may help reduce the influence of outliers. (We'll come back to this!)

--
3. **Replace them**
    
    - Replacing the outliers with values `\(\pm\)` 2-3 standard deviations away from the mean. 


---
class: center, inverse, middle
# Identifying and replacing outliers

---
# Identifying outliers

.pull-left[

```
## `geom_smooth()` using formula = 'y ~ x'
```

![](B-1-messy-data_files/figure-html/unnamed-chunk-10-1.png)&lt;!-- --&gt;
]
.pull-right[
Plotting your data can be an excellent way to spot outliers: here they're *very* obvious!
]

---
# Identifying outliers

Plotting the residuals of your linear model will also help you identify troublesome observations.

```r
plot(lm(X1 ~ X2, data = temp_df_out))
```
.pull-left[
![](B-1-messy-data_files/figure-html/unnamed-chunk-12-1.png)&lt;!-- --&gt;
]
.pull-right[
![](B-1-messy-data_files/figure-html/unnamed-chunk-13-1.png)&lt;!-- --&gt;
]

---
# Identifying outliers

Plotting the residuals of your linear model will also help you identify troublesome observations.
.pull-left[
![](B-1-messy-data_files/figure-html/unnamed-chunk-14-1.png)&lt;!-- --&gt;
]
.pull-right[
![](B-1-messy-data_files/figure-html/unnamed-chunk-15-1.png)&lt;!-- --&gt;
]

---
# Identifying outliers

The `performance` library has a `check_outliers()` function that helps too!


```r
library(performance)
```

```
## Warning: package 'performance' was built under R version 4.2.2
```

```r
plot(check_outliers(lm(X1 ~ X2, data = temp_df_out)))
```

![](B-1-messy-data_files/figure-html/unnamed-chunk-16-1.png)&lt;!-- --&gt;

---
# Identifying outliers

.pull-left[
Sometimes a *threshold* is used to determine whether an observation is an outlier.

Sometimes this is driven by common sense: e.g. a value of 120 for a participant's age is **extremely** unlikely to be genuine.

Sometimes this is *data-driven*: e.g. values more than `\(\pm\)` 3 standard deviations away from the mean are *unusual*.
]
.pull-right[
![](B-1-messy-data_files/figure-html/unnamed-chunk-17-1.png)&lt;!-- --&gt;
]


---
# Scaling and standardizing

.panelset[
.panel[.panel-name[The data]

```r
example &lt;- c(rnorm(15), 35)
hist(example)
```

![](B-1-messy-data_files/figure-html/unnamed-chunk-18-1.png)&lt;!-- --&gt;
]
.panel[.panel-name[Manually scale]

```r
example
```

```
##  [1] -1.0041780 -0.5334676  1.3035305  1.2248651 -0.8032487 -0.8871387
##  [7]  0.8811937  0.4287084 -1.8326358  0.9050068 -2.0932331 -0.8987411
## [13] -1.2179617  0.4069258  0.2064533 35.0000000
```

```r
(example - mean(example)) / sd(example)
```

```
##  [1] -0.33191363 -0.27889968 -0.07200700 -0.08086672 -0.30928389 -0.31873203
##  [7] -0.11957286 -0.17053420 -0.42521904 -0.11689089 -0.45456892 -0.32003875
## [13] -0.35599111 -0.17298748 -0.19556577  3.72307198
```
]
.panel[.panel-name[`scale()`]

```r
# you don't need to use t() - that transposes the values so they fit on the slide :) 
# Just use scale()
t(scale(example)) 
```

```
##            [,1]       [,2]      [,3]        [,4]       [,5]      [,6]
## [1,] -0.3319136 -0.2788997 -0.072007 -0.08086672 -0.3092839 -0.318732
##            [,7]       [,8]      [,9]      [,10]      [,11]      [,12]
## [1,] -0.1195729 -0.1705342 -0.425219 -0.1168909 -0.4545689 -0.3200388
##           [,13]      [,14]      [,15]    [,16]
## [1,] -0.3559911 -0.1729875 -0.1955658 3.723072
## attr(,"scaled:center")
## [1] 1.94288
## attr(,"scaled:scale")
## [1] 8.87899
```
]
]

---
# Removing values above a threshold

The `filter()` function from dplyr can be used to remove outliers easily!
.panelset[
.panel[.panel-name[With outliers]
.pull-left[

```r
temp_df_out %&gt;%
  ggplot(aes(x = X1, y = X2)) + 
  geom_point() +
  theme_bw() + 
  stat_smooth(method = "lm")
```

```
## `geom_smooth()` using formula = 'y ~ x'
```
]
.pull-right[
![](B-1-messy-data_files/figure-html/plot-with-1.png)
]
]
.panel[
.panel-name[Without outliers]
.pull-left[

```r
temp_df_out %&gt;%
  dplyr::filter(X1 &lt; 15) %&gt;%
  ggplot(aes(x = X1, y = X2)) + 
  geom_point() + theme_bw() + 
  stat_smooth(method = "lm")
```

```
## `geom_smooth()` using formula = 'y ~ x'
```
]
.pull-right[
![](B-1-messy-data_files/figure-html/plot-without-1.png)
]
]
]

---
class: center, inverse, middle
# Data transformation

---
# Skewed data

Skewed data is data that *leans* in a particular direction. 

These are often described by the direction of the "long-tail" - so a left-tailed distribution means a distribution with a long tail on the left, rather than most values on the left.
.center[
![](B-1-messy-data_files/figure-html/unnamed-chunk-21-1.png)&lt;!-- --&gt;
]

---
# Skewed data

.pull-left[

```
## Warning: `qplot()` was deprecated in ggplot2 3.4.0.
```

![](B-1-messy-data_files/figure-html/unnamed-chunk-22-1.png)&lt;!-- --&gt;
]
.pull-right[
This data is *right-tailed*. This is sometimes also called *positively skewed*. For this type of data, the mean (blue line) is usually higher than the median (red, dashed line).

This type of skew is relatively common with data that is *bounded* at zero. e.g. reaction time data, the distribution of wages 
]

---
# Left-tailed skew

.pull-left[
![](B-1-messy-data_files/figure-html/unnamed-chunk-23-1.png)&lt;!-- --&gt;
]
.pull-right[
Data skewed the opposite way - many high scores but few low scores - has a long *left* tail. This is also called *negative skew*. The mean (blue solid line) is usually less than the median (red dashed line).
]

---
# Transformation of skewed data

One way to handle skew is to transform the data to a different *scale*.

Transformation type| code|
-------|---
Log|log(X)
Square root|sqrt(X)
Reciprocal | 1 / X
Square | x^2

(See Section 5.8.2 in Field et al., DSUR)
---
# Transformation of skewed data

.panelset[
.panel[.panel-name[Right-tailed]
![](B-1-messy-data_files/figure-html/unnamed-chunk-24-1.png)&lt;!-- --&gt;
]
.panel[.panel-name[Left-tailed]
![](B-1-messy-data_files/figure-html/unnamed-chunk-25-1.png)&lt;!-- --&gt;
]
]

---
class: center, inverse, middle
# Handling missing data

---
# Types of missing data

.pull-left[
- Missing Completely At Random
    - Missingness does not depend on anything

- Missing At Random
    - Missingness depends on the observed data

- Missing Not At Random
    - Missingness depends on the missing data
]
.pull-right[
&lt;blockquote class="twitter-tweet"&gt;&lt;p lang="en" dir="ltr"&gt;In today&amp;#39;s lecture, I tried to redefine missing data types (MCAR, MAR, MNAR) as different reasons a dog might eat your homework. This needs more work, but audience seemed to appreciate it. &lt;a href="https://t.co/i9Z8sYrqWQ"&gt;pic.twitter.com/i9Z8sYrqWQ&lt;/a&gt;&lt;/p&gt;&amp;mdash; Richard McElreath (@rlmcelreath) &lt;a href="https://twitter.com/rlmcelreath/status/1101435108995805185?ref_src=twsrc%5Etfw"&gt;March 1, 2019&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;
]

---
# Missing Completely At Random

.pull-left[
If you have missing data, MCAR is the best kind of missing data. 

There is nothing *systematic* about which data is missing. 

For example, all your participants filled out three different questionnaires. Unfortunately, your dog chewed through a pile of them, and half of your participants now have only two questionnaires.
]
.pull-right[

```
##           X1        X2       X3
## 1         NA 10.564509 12.00366
## 2         NA 11.134840 12.46372
## 3         NA  8.197005 11.06698
## 4         NA  9.704088 14.19018
## 5         NA  8.706921 13.03839
## 6   8.344476  9.537686 12.89438
## 7  11.263166  7.791435 13.76203
## 8   8.669327  8.276546 12.92337
## 9   8.377079  9.164257 11.07893
## 10  9.288548  6.947327 12.80245
```
]

---
# Missing At Random

.pull-left[

```
##           X1        X2       X3 age
## 1  11.047154  9.130396 12.86073  19
## 2   8.509500 10.760188 12.07602  20
## 3  11.619125  9.636584 13.04400  19
## 4  11.212559  7.600109 11.90494  20
## 5  10.951217  9.341155 13.35871  19
## 6   9.286955 10.227833 12.48842  19
## 7   9.555602  8.320163       NA  36
## 8  10.394811  7.726169 15.13988  19
## 9   8.376943  8.289583       NA  26
## 10 10.528843  8.812628 13.50900  20
```
]
.pull-right[
Confusingly, Missing At Random (MAR) data is not missing (completely) at random. 

For example, for some reason, people older than 21 typically failed to complete the third questionnaire.

This data is MAR - whether the data in the third column is missing is related to the value of the fourth column.
]

---
# Missing Not At Random

.pull-left[

```
##           X1        X2       X3 age
## 1  11.095201  8.235348       NA  19
## 2  11.159246  9.198542       NA  19
## 3  11.026114 11.395065       NA  29
## 4  10.723638 10.066800       NA  19
## 5  11.399608 11.517779       NA  19
## 6  10.051001  9.103957 12.61627  18
## 7   9.073119  7.654592 13.10197  42
## 8  10.110520 10.048114 12.38895  29
## 9   9.729318 10.017504       NA  19
## 10 11.901271  9.132584 13.14818  52
```
]
.pull-right[
The final, most troubling type of missing data is data that is Missing Not At Random (MNAR).

For example, imagine that the questionnaire relates to depression; people who score high for depression are less likely to complete the final questionnaire.

In this case, the values that are missing for the third questionnaire depends on the value of the responses to that questionnaire, so this data is MNAR.
]

---
# Dealing with missing data

.pull-left[
List-wise deletion: Cases with missing data are completely **removed** from **all** analysis.

Pair-wise deletion: Cases with missing data are only **removed** from **comparisons where one or more variables are missing**.

By default, functions such as `mean()` return NA if any value in the input is NA/missing.
]
.pull-right[

```r
mean(temp_df_missing$X1)
```

```
## [1] NA
```

```r
mean(temp_df_missing$X1, na.rm = TRUE)
```

```
## [1] 9.998442
```

```r
sum(complete.cases(temp_df_missing))
```

```
## [1] 193
```
]

---
# Single Imputation

Replace missing values with a simple "best-guess". e.g. Using the mean or the median for the condition.

.pull-left[

```r
orig_data &lt;- 1:12
orig_data
```

```
##  [1]  1  2  3  4  5  6  7  8  9 10 11 12
```

```r
mean(orig_data)
```

```
## [1] 6.5
```
]
.pull-right[

```r
missing_one &lt;- orig_data
missing_one[6] &lt;- NA
missing_one
```

```
##  [1]  1  2  3  4  5 NA  7  8  9 10 11 12
```

```r
mean(missing_one,  na.rm = TRUE)
```

```
## [1] 6.545455
```
]

---
# Single Imputation

Replace missing values with a simple "best-guess". e.g. Using the mean or the median for the condition.

.pull-left[

```r
replace_one &lt;- missing_one
replace_one[6] &lt;- mean(missing_one,
                       na.rm = TRUE)
mean(replace_one)
```

```
## [1] 6.545455
```

```r
mean(orig_data)
```

```
## [1] 6.5
```
]

.pull-right[
Problem: the mean and median are biased by the missing data. And replacing a missing value with one of these values tends to artificially reduce variability.
]

---

## Multiple Imputation

In **multiple** imputation, we replace missing values with estimates based on a *model* of the data that incorporates uncertainty about what the value should be. 

We create a model based on the data that is not missing, and use its predictions to guess the values that the missing data has.

We do this multiple times and then take an average or *pool* the results to fill in the gap.

Packages such as `mice` and `Amelia` can do this for us, and help us identify patterns of missingness.

---
# Alternative approaches to missing data, skew, and other oddities

Generalized Linear Models (as opposed to General Linear Models) allow modelling of data of many different types without necessitating transformations. 

For example, counts can be modelled using Poisson regression, and categorical outcomes can be modelled with logistic regression.

*Multilevel* or *mixed*-models can handle all of these things and much more besides; they are perfectly capable of handling missing data.

We'll cover both logistic regression and multilevel models later in the course!

---
# Real-life example

Data collected as part of a Newfoundland Symphony Orchestra Concert to study memory for new music as a function of age and familiarity (tonal/atonal, familiar, unfamiliar).

1. Clicker data
2. Questionnaire (contains age information)
3. Cognitive tests

How would you address the missing data?

---
# Next week

Look into power and effect sizes:

See Field et al, Discovering Statistics Using R, pages 56-59, Sections on:
    - Type I and Type II error (2.6.3)
    - effect sizes (2.6.4)
    - statistical power (2.6.5)

Cohen, J. (1992). A power primer. Psychological Bulletin, 112(1), 155-159.
http://dx.doi.org/10.1037/0033-2909.112.1.155

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="js/macros.js"></script>
<script src="https://platform.twitter.com/widgets.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
